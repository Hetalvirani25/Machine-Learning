# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PH8epltWi2bMU-2DDwSY4DYZKVum5TNG

# Lab1 - Scikit-learn
Author: *YOUR NAME*

#### Due Date: Sunday, February 12 at 11:59pm

#### To submit: Google Colab link for the completed code file

## 1. Introduction

The goal of this lab is to become more familiar with the scikit-learn library

You will practice loading example datasets, perform classification and regression with linear scikit-learn models, and investigate the effects of reducing the number of features (columns in X) and the number of samples (rows in X and y)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

!pip install mglearn
!pip install utils

import utils

!pip install --upgrade joblib==1.1.0

import mglearn

mglearn.plots.plot_linear_regression_wave()

import warnings
warnings.filterwarnings('ignore')

"""## 2. Classification (15 marks total)

Using spam dataset - classification  
https://archive.ics.uci.edu/ml/datasets/spambase

The goal is to investigate `LogisticRegression(max_iter=2000)` and the effects of reducing the number of features and number of samples on classification performance

### 2.1 Load data (4 marks)

#### First step: Check first few lines of data file using !head or !type

Hint: Use this to check what the separator (sep) is and if column headers are included
"""

# TODO: ADD YOUR CODE HERE (0.5 marks)
!head spambase.data

"""#### Second step: Read in csv file

Hint 1: From the previous step, what separator should be used?

Hint 2: If there are no column headers, how does this change how we read in the data? 

Hint 3: If there are no column headers, do not need to define them - can index columns using numerical index
"""

# TODO: ADD YOUR CODE HERE (1 mark)
data = pd.read_csv('spambase.data',na_values='?')
df=pd.DataFrame(data)

df.columns=[i for i in range(58)]
print(df)

"""#### Third step: Find any null values and use a reasonable method to deal with them"""

# TODO: ADD YOUR CODE HERE (0.5 marks)
df.dtypes
df.isnull().sum()

"""#### Fourth step: Split dataset into feature matrix `X` and target vector `y`
Print dimensions and type of `X` and `y`
"""

# TODO: ADD YOUR CODE HERE (1 mark)

X, y = df.iloc[:,0:49],df.iloc[:,49:59]
print("X-dimension",X.shape)
# print(X.dtypes)
# print('----------------------------------------------')
# print('----------------------------------------------')
print("y-dimension",y.shape)
# print(y.dtypes)

"""Using the sklearn function `train_test_split()` prepare an additional feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`

Print dimensions and type of `X_small` and `y_small`
"""

# TODO: ADD YOUR CODE HERE (1 mark)

from sklearn.model_selection import train_test_split
X, y = df.iloc[:,0:49],df.iloc[:,49:59]
X_train, X_small, y_train, y_small = train_test_split(X, y, test_size=0.01, random_state=174)

print("X-small-dimension",X_small.shape)
# print(X_small.dtypes)
print("y_small-dimension",y_small.shape)
# print(y_small.dtypes)

"""### 2.2 Implement accuracy function (3 marks)"""

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing

def get_classifier_accuracy(model, X, y):
  X_train, X_small, y_train, y_small = train_test_split(X, y,test_size=0.01, random_state=956)
  model.fit(X_train,y_train)
  y_pred=model.predict(X_small)
  validation_accuracy = np.mean(y_pred == y_small)
  y_train_pred=model.predict(X_train)
  train_accuracy=np.mean(y_train_pred == y_train)
  return train_accuracy,validation_accuracy


 
  
X, y = df.iloc[:,0:49],df.iloc[:,57]

model = LogisticRegression(max_iter=2000)


train_accuracy, validation_accuracy = get_classifier_accuracy(model, X,y)

print("train_accuracy",train_accuracy)

print("validation_accuracy",validation_accuracy)

"""### 2.3 Train and evaluate models (5 marks)

1. Import `LogisticRegression` from sklearn
2. Instantiate model `LogisticRegression(max_iter=2000)`.
3. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy
4. Call your accuracy function `get_classifier_accuracy()` using 
    - `X` and `y`
    - Only first two columns of `X` and `y`
    - `X_small` and `y_small`
5. Add the data size, training and validation accuracy for each call to the `results` DataFrame
6. Print `results`
"""

# TODO: ADD YOUR CODE HERE

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing

def get_classifier_accuracy(model, X, y):
  X_train, X_small, y_train, y_small = train_test_split(X, y,test_size=0.01, random_state=956)
  model.fit(X,y)
  y_pred=model.predict(X_small)
  validation_accuracy=np.mean(y_pred == y_small)
  y_train_pred=model.predict(X_train)
  train_accuracy=np.mean(y_train_pred == y_train)
  return train_accuracy,validation_accuracy

results = pd.DataFrame(columns=['Data size', 'Training accuracy', 'Validation accuracy'])
 
  
X, y = df.iloc[:,0:49],df.iloc[:,57]

model = LogisticRegression(max_iter=2000)

data_size=X.shape[0]

train_accuracy, val_accuracy = get_classifier_accuracy(model, X,y)


results = results.append({'Data size': data_size,
                          'Training accuracy': train_accuracy,
                          'Validation accuracy': val_accuracy}, ignore_index=True)

x_sample=X.iloc[:, :3]

data_size=x_sample.shape[0]

train_accuracy, val_accuracy = get_classifier_accuracy(model,x_sample, y)


results = results.append({'Data size': data_size,
                          'Training accuracy': train_accuracy,
                          'Validation accuracy': val_accuracy}, ignore_index=True)


X_train, X_small, y_train, y_small = train_test_split(X, y,test_size=0.01, random_state=956)

data_size=X_small.shape[0]
train_accuracy,validation_accuracy = get_classifier_accuracy(model,X_small,y_small)

results = results.append({'Data size': data_size,
                          'Training accuracy': train_accuracy,
                          'Validation accuracy': val_accuracy}, ignore_index=True)

   
print(results)

"""### 2.4 Questions (3 marks)
1. What is the validation accuracy using all data? What is the difference between training and validation accuracy?
1. How does the validation accuracy and difference between training and validation change when only two columns are used? Provide values.
1. How does the validation accuracy and difference between training and validation change when only 1% of the rows are used? Provide values.

*YOUR ANSWERS HERE*

1) When a model's predictions on a validation dataset are compared to the actual labels in that dataset, the accuracy of the model's predictions is said to be the validation accuracy utilising all data.

The training accuracy of a model refers to its accuracy on the training dataset that it saw throughout the training phase, while the validation accuracy of a model refers to its performance on a validation dataset that is not viewed during training.


2) Depending on the importance of the columns to the target variable and the complexity of the model, using merely two columns in a dataset may have an impact on the validation accuracy and the difference between training and validation accuracy.

Even if the two columns include very essential data, the model might still be able to achieve good validation accuracy with just those two columns.

Data volume 460.0 Training effectiveness 0.635266 Validation precision is 0.673913.

3) Since fewer data are available for the model's training and verification, it may be less able to make accurate predictions if only 1% of the rows are used.

The model will be trained on a skewed sample and will perform poorly on the validation data if the 1% of rows selected are not representative of the entire dataset.

Data volume 460.0 Training precision is 0.916776 Validation precision is 0.891304.

## 3. Regression (13 marks total)

Using energy efficiency dataset - regression  
http://archive.ics.uci.edu/ml/datasets/Energy+efficiency

The goal is to investigate `LinearRegression()` and the effects of reducing the number of features and number of samples on regression performance

### 3.1 Load data (2 marks)

In the previous example, we downloaded the dataset from the UCI repository and read in the data using pandas

Using sci-kit learn, you can access multiple datasets from UCI using the yellowbrick function (https://www.scikit-yb.org/en/latest/index.html)

To install yellowbrick, use: `pip install yellowbrick`

To access the energy efficiency dataset, visit: https://www.scikit-yb.org/en/latest/api/datasets/energy.html

Using the yellowbrick function `load_energy()`, load the energy dataset into feature matrix `X` and target vector `y`

Print dimensions and type of `X` and `y`
"""

!pip install yellowbrick

# TODO: ADD YOUR CODE HERE (1 mark)

from yellowbrick.datasets import load_energy
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
df2 = pd.read_excel('ENB2012_data.xlsx')
df2.columns=["relative compactness","surface area","wall area","roof area","overall height","orientation","glazing area","glazing area distribution","heating load", "cooling load"]

features = [
   "relative compactness",
   "surface area",
   "wall area",
   "roof area",
   "overall height",
   "orientation",
   "glazing area",
   "glazing area distribution"
]
target = ["heating load", "cooling load"]
df2 = load_energy(return_dataset=True).to_dataframe()
X, y = df2[features], df2[target]

X.shape
y.shape

"""Using the sklearn function `train_test_split()` prepare an additional feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`

Print dimensions and type of `X_small` and `y_small`
"""

# TODO: ADD YOUR CODE HERE (1 mark)

X_train, X_small, y_train, y_small = train_test_split(X, y, test_size=0.01,random_state=174)

print(X_small.shape)
print(X_small.dtypes)
print('______________________________')
print(y_small.shape)

print(y_small.dtypes)

"""### 3.2 Implement accuracy function (3 marks)"""

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
def get_regressor_mse(model, X, y):

   X_train, X_small, y_train, y_small = train_test_split(X, y,test_size=0.01, random_state=174)
   model.fit(X_train,y_train)
   y_pred=model.predict(X_small)
   valid_mse = mean_squared_error(y_small,y_pred)
   y_train_pred=model.predict(X_train)
   train_mse=mean_squared_error(y_train,y_train_pred)
   return train_mse,valid_mse

X, y = df2[features], df2[target]
model=LinearRegression()
train_mse, valid_mse = get_regressor_mse(model, X,y)

print("train_mse",train_mse)
print("validation_mse",valid_mse)


    #TODO: IMPLEMENT FUNCTION BODY

"""### 3.3 Train and evaluate models (5 marks)

1. Import `LinearRegression` from sklearn
2. Instantiate model `LinearRegression()`.
3. Create a pandas DataFrame `results` with columns: Data size, training MSE, validation MSE
4. Call your accuracy function `get_regressor_mse()` using 
    - `X` and `y`
    - Only first two columns of `X` and `y`
    - `X_small` and `y_small`
5. Add the data size, training and validation MSE for each call to the `results` DataFrame
6. Print `results`
"""

# TODO: ADD YOUR CODE HERE

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import preprocessing

def get_regressor_mse(model, X, y):

   X_train, X_small, y_train, y_small = train_test_split(X, y,test_size=0.01, random_state=174)
   model.fit(X_train,y_train)
   y_pred=model.predict(X_small)
   valid_mse = mean_squared_error(y_small,y_pred)
   y_train_pred=model.predict(X_train)
   train_mse=mean_squared_error(y_train,y_train_pred)
   return train_mse,valid_mse

results = pd.DataFrame(columns=['Data size', 'Training MSE', 'Validation MSE'])
 
  
X, y = df2[features], df2[target]

model = LinearRegression()

data_size=X.shape[0]

train_mse, valid_mse = get_regressor_mse(model, X,y)

results = results.append({'Data size': data_size,
                          'Training MSE': train_accuracy,
                          'Validation MSE': val_accuracy}, ignore_index=True)

x_sample=X.iloc[:, :3]

data_size=x_sample.shape[0]

train_mse, valid_mse = get_regressor_mse(model, x_sample,y)


results = results.append({'Data size': data_size,
                          'Training MSE': train_accuracy,
                          'Validation MSE': val_accuracy}, ignore_index=True)

data_size=X_small.shape[0]
train_mse, valid_mse = get_regressor_mse(model, X_small,y_small)

results = results.append({'Data size': data_size,
                          'Training MSE': train_accuracy,
                          'Validation MSE': val_accuracy}, ignore_index=True)


print(results)

"""### 3.4 Questions (3 marks)
1. What is the validation MSE using all data? What is the difference between training and validation MSE?
1. How does the validation MSE and difference between training and validation change when only two columns are used? Provide values.
1. How does the validation MSE and difference between training and validation change when only 1% of the rows are used? Provide values.

*YOUR ANSWERS HERE*

1) The average squared difference between the model's predicted values and the actual values in the validation dataset is referred to as the validation Mean Squared Error (MSE) using all data. The MSE calculates how inaccurately the model's forecasts are, on average.

The average squared difference between the model's predicted values and the actual values in the training dataset that were noticed during training is known as the training MSE. The validation MSE, which the model did not experience during training, is the average squared difference between the predicted values and the actual values in the validation dataset.

2) The model might only have a low validation MSE if the two columns include highly pertinent data. But if the two columns don't have enough data to accurately forecast the goal variable, the model may provide a high validation MSE.

3) The amount of data available for the model's training and verification decreases when only 1% of the rows are used, which may have an impact on the model's ability to make accurate predictions.

The model will be trained on a skewed sample and will perform poorly on the validation data if the 1% of rows selected are not representative of the entire dataset.

Validation mse 5.897659522510363, Train mse 9.429085570183652

## 4. Observations/Interpretation (3 marks)

Describe any patterns you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.


*ADD YOUR FINDINGS HERE*

Using reduced feature data and its effects

If we restrict the feature space from which the model may learn, this may affect how well the predictions on validation data turn out.

The model may overfit the training data if the chosen data is highly associated with the target variable and perform badly on the validation data.

Underfitting: The model underfits the training data and performs badly on both the training and validation data if the chosen data is insufficient to predict the target variable.

## 5. Reflection (2 marks)
Include a sentence or two about:
- what you liked or disliked,
- found interesting, confusing, challangeing, motivating
while working on this assignment.


*ADD YOUR THOUGHTS HERE*

Class notes are very helpful in completing this assignment because it provides a deeper understanding of logistic and linear regression models. 

I learned more practical information about how to train and evaluate the model, how smaller data affect training and validation accuracy, and how to split a dataset into a feature and a target vector how to determine the model's correctness how to determine MSE which models suit the various data sets the best.
"""